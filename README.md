# Reinforcement_Learning

![image](https://github.com/kundankr4/Reinforcement_Learning/assets/126001733/66526c5e-dd76-4945-90c6-7176c4c072b0)

## Exploratory Analysis of Reinforcement Learning Algorithms: PPO, A2C, and DQN in Cart-Pole Environment
## Overview
The "Cartpole Balancing with Reinforcement Learning" project explores the implementation of advanced reinforcement learning (RL) algorithms to solve the Cartpole balancing problem. This project showcases the utilization of popular RL algorithms like Proximal Policy Optimization (PPO), Advantage Actor-Critic (A2C), and Deep Q-Network (DQN) in a simulated environment. The goal is to balance a pole on a moving cart by applying appropriate forces to the cart.

# Key Features
Algorithm Implementation: Integration of PPO, A2C, and DQN algorithms for the Cartpole balancing problem.
Environment Testing: Utilization of the Gymnasium framework to simulate the Cartpole environment for testing and training the RL models.
Model Training and Evaluation: Training of RL models using different strategies and evaluating their performance based on reward and stability.
TensorBoard Visualization: Integration with TensorBoard for real-time visualization of training progress and model performance metrics.
Model Comparison: Comparative analysis of different RL algorithms based on their learning curves and performance in the simulated environment.

# Usage
The project consists of Python notebooks/scripts that demonstrate the process of initializing the environment, training the RL models, and evaluating their performance. Users can experiment with different model parameters and training settings.

# Contribution
Contributions to enhance the models, add new features, or improve documentation are welcome. Please feel free to fork the project and create a pull request with your changes.

# License
This project is licensed under Apache License 2.0 .

# Acknowledgments
Special thanks to the creators of Gymnasium, Stable Baselines3, and other libraries used in this project. The project also acknowledges the valuable resources and documentation provided by OpenAI and Hugging Face.



